{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Problem_B5.ipynb","provenance":[],"authorship_tag":"ABX9TyOJGQfxM/URQTfQQHZyz/X3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sjFTV2EEOVhV","executionInfo":{"status":"ok","timestamp":1655805364396,"user_tz":-420,"elapsed":197275,"user":{"displayName":"KEVIN ADRIAN HALIM","userId":"18434347424817665992"}},"outputId":"32c4d947-c421-44ec-fdb6-dbe6b053a459"},"outputs":[{"output_type":"stream","name":"stdout","text":["<PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 1), dtype=tf.float64, name=None), TensorSpec(shape=(None, None, 1), dtype=tf.float64, name=None))>\n","(2500,)\n","Epoch 1/20\n","10/10 [==============================] - 15s 908ms/step - loss: 0.0270 - mae: 0.1890 - val_loss: 0.0104 - val_mae: 0.1028\n","Epoch 2/20\n","10/10 [==============================] - 8s 773ms/step - loss: 0.0098 - mae: 0.1012 - val_loss: 0.0086 - val_mae: 0.1063\n","Epoch 3/20\n","10/10 [==============================] - 8s 774ms/step - loss: 0.0083 - mae: 0.1005 - val_loss: 0.0085 - val_mae: 0.0933\n","Epoch 4/20\n","10/10 [==============================] - 8s 772ms/step - loss: 0.0077 - mae: 0.0962 - val_loss: 0.0073 - val_mae: 0.0900\n","Epoch 5/20\n","10/10 [==============================] - 8s 773ms/step - loss: 0.0069 - mae: 0.0898 - val_loss: 0.0064 - val_mae: 0.0833\n","Epoch 6/20\n","10/10 [==============================] - 8s 773ms/step - loss: 0.0061 - mae: 0.0839 - val_loss: 0.0061 - val_mae: 0.0836\n","Epoch 7/20\n","10/10 [==============================] - 8s 769ms/step - loss: 0.0059 - mae: 0.0831 - val_loss: 0.0058 - val_mae: 0.0807\n","Epoch 8/20\n","10/10 [==============================] - 8s 776ms/step - loss: 0.0058 - mae: 0.0827 - val_loss: 0.0057 - val_mae: 0.0809\n","Epoch 9/20\n","10/10 [==============================] - 8s 775ms/step - loss: 0.0057 - mae: 0.0820 - val_loss: 0.0056 - val_mae: 0.0806\n","Epoch 10/20\n","10/10 [==============================] - 8s 773ms/step - loss: 0.0057 - mae: 0.0818 - val_loss: 0.0056 - val_mae: 0.0805\n","Epoch 11/20\n","10/10 [==============================] - 8s 769ms/step - loss: 0.0057 - mae: 0.0817 - val_loss: 0.0056 - val_mae: 0.0791\n","Epoch 12/20\n","10/10 [==============================] - 8s 772ms/step - loss: 0.0056 - mae: 0.0813 - val_loss: 0.0055 - val_mae: 0.0797\n","Epoch 13/20\n","10/10 [==============================] - 8s 769ms/step - loss: 0.0056 - mae: 0.0811 - val_loss: 0.0055 - val_mae: 0.0793\n","Epoch 14/20\n","10/10 [==============================] - 8s 773ms/step - loss: 0.0056 - mae: 0.0809 - val_loss: 0.0054 - val_mae: 0.0792\n","Epoch 15/20\n","10/10 [==============================] - 8s 771ms/step - loss: 0.0056 - mae: 0.0809 - val_loss: 0.0054 - val_mae: 0.0788\n","Epoch 16/20\n","10/10 [==============================] - 8s 778ms/step - loss: 0.0055 - mae: 0.0807 - val_loss: 0.0054 - val_mae: 0.0785\n","Epoch 17/20\n","10/10 [==============================] - 8s 772ms/step - loss: 0.0055 - mae: 0.0805 - val_loss: 0.0054 - val_mae: 0.0775\n","Epoch 18/20\n","10/10 [==============================] - 8s 774ms/step - loss: 0.0055 - mae: 0.0802 - val_loss: 0.0053 - val_mae: 0.0785\n","Epoch 19/20\n","10/10 [==============================] - 8s 776ms/step - loss: 0.0055 - mae: 0.0803 - val_loss: 0.0053 - val_mae: 0.0773\n","Epoch 20/20\n","10/10 [==============================] - 8s 771ms/step - loss: 0.0054 - mae: 0.0799 - val_loss: 0.0052 - val_mae: 0.0780\n"]}],"source":["# ============================================================================================\n","# PROBLEM B5\n","#\n","# Build and train a neural network model using the Daily Max Temperature.csv dataset.\n","# Use MAE as the metrics of your neural network model.\n","# We provided code for normalizing the data. Please do not change the code.\n","# Do not use lambda layers in your model.\n","#\n","# The dataset used in this problem is downloaded from https://github.com/jbrownlee/Datasets\n","#\n","# Desired MAE < 0.2 on the normalized dataset.\n","# ============================================================================================\n","\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import csv\n","import urllib\n","import pandas as pd\n","from tensorflow.keras.optimizers import Adam\n","\n","\n","def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n","    series = tf.expand_dims(series, axis=-1)\n","    ds = tf.data.Dataset.from_tensor_slices(series)\n","    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n","    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n","    ds = ds.shuffle(shuffle_buffer)\n","    ds = ds.map(lambda w: (w[:-1], w[1:]))\n","    return ds.batch(batch_size).prefetch(1)\n","\n","\n","def solution_B5():\n","    df = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-max-temperatures.csv')\n","    series = df[\"Temperature\"]\n","    # Normalization Function. DO NOT CHANGE THIS CODE\n","    min=np.min(series)\n","    max=np.max(series)\n","    series -= min\n","    series /= max\n","   # time=np.array(time_step)\n","\n","    # DO NOT CHANGE THIS CODE\n","    split_time=2500\n","\n","    x_train=series[:2500]\n","    x_valid= series[2500:]\n","\n","    # DO NOT CHANGE THIS CODE\n","    window_size=64\n","    batch_size=256\n","    shuffle_buffer_size=1000\n","\n","    train_set=windowed_dataset(\n","        x_train, window_size, batch_size, shuffle_buffer_size)\n","    print(train_set)\n","    print(x_train.shape)\n","    valid_set = windowed_dataset(x_valid,window_size,batch_size,shuffle_buffer_size)\n","\n","    model=tf.keras.models.Sequential([\n","        tf.keras.layers.Conv1D(16,3, strides =1,\n","                                padding = \"causal\",\n","                               activation = \"relu\",\n","                               input_shape = [None,1]),\n","        tf.keras.layers.LSTM(64,return_sequences = True),\n","        tf.keras.layers.LSTM(64,return_sequences = True),\n","        tf.keras.layers.LSTM(128,return_sequences = True),\n","        tf.keras.layers.Dense(64,activation = \"relu\"),\n","        tf.keras.layers.Dense(128,activation = \"relu\"),\n","        tf.keras.layers.Dense(1),\n","    ])\n","\n","    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n","      lambda epoch: 1e-8 * 10**(epoch / 20))\n","\n","    model.compile(loss = tf.keras.losses.Huber(),\n","                  optimizer = Adam(learning_rate = 0.0001),\n","                  metrics =[\"mae\"])\n","    #model.fit(train_set,epochs = 100,verbose = 1 ,callbacks = [lr_schedule])\n","    # YOUR CODE HERE\n","\n","    model.compile(loss = tf.keras.losses.Huber(),\n","                  optimizer =Adam(learning_rate = 0.001),\n","                  metrics = [\"mae\"])\n","    model.fit(train_set,\n","              epochs = 20,\n","              verbose = 1 ,\n","              validation_data = valid_set)\n","    return model\n","# The code below is to save your model as a .h5 file.\n","# It will be saved automatically in your Submission folder.\n","if __name__ == '__main__':\n","    # DO NOT CHANGE THIS CODE\n","    model=solution_B5()\n","    model.save(\"model_B5.h5\")"]},{"cell_type":"code","source":[""],"metadata":{"id":"YniQGq6Xsy9K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"JmYQkCk5OnD-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Ep7PepgXPamg"},"execution_count":null,"outputs":[]}]}