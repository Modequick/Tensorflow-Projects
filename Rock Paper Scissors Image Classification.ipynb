{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Problem_B3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP873f0aucvnDPkAD5D4T0r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BpqNe5zJcLD9","executionInfo":{"status":"ok","timestamp":1655903253401,"user_tz":-420,"elapsed":1306979,"user":{"displayName":"KEVIN ADRIAN HALIM","userId":"18434347424817665992"}},"outputId":"9aa080be-8a35-4730-c523-943cf4f13481"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2268 images belonging to 3 classes.\n","Found 252 images belonging to 3 classes.\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 148, 148, 16)      448       \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 74, 74, 16)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 72, 72, 32)        4640      \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 36, 36, 32)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 34, 34, 64)        18496     \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 17, 17, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 15, 15, 64)        36928     \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 7, 7, 64)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 3136)              0         \n","                                                                 \n"," dropout (Dropout)           (None, 3136)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               1606144   \n","                                                                 \n"," dense_1 (Dense)             (None, 3)                 1539      \n","                                                                 \n","=================================================================\n","Total params: 1,668,195\n","Trainable params: 1,668,195\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/100\n","32/32 [==============================] - 30s 906ms/step - loss: 0.2230 - accuracy: 0.3203 - val_loss: 0.2205 - val_accuracy: 0.3889\n","Epoch 2/100\n","32/32 [==============================] - 29s 892ms/step - loss: 0.2205 - accuracy: 0.3926 - val_loss: 0.2196 - val_accuracy: 0.2778\n","Epoch 3/100\n","32/32 [==============================] - 29s 894ms/step - loss: 0.2186 - accuracy: 0.3980 - val_loss: 0.2083 - val_accuracy: 0.5694\n","Epoch 4/100\n","32/32 [==============================] - 29s 897ms/step - loss: 0.2134 - accuracy: 0.4402 - val_loss: 0.1984 - val_accuracy: 0.7083\n","Epoch 5/100\n","32/32 [==============================] - 29s 900ms/step - loss: 0.2075 - accuracy: 0.4639 - val_loss: 0.1969 - val_accuracy: 0.4583\n","Epoch 6/100\n","32/32 [==============================] - 29s 887ms/step - loss: 0.1951 - accuracy: 0.5216 - val_loss: 0.1700 - val_accuracy: 0.6944\n","Epoch 7/100\n","32/32 [==============================] - 30s 932ms/step - loss: 0.1860 - accuracy: 0.5500 - val_loss: 0.1667 - val_accuracy: 0.6667\n","Epoch 8/100\n","32/32 [==============================] - 29s 894ms/step - loss: 0.1768 - accuracy: 0.5667 - val_loss: 0.2158 - val_accuracy: 0.5000\n","Epoch 9/100\n","32/32 [==============================] - 29s 897ms/step - loss: 0.1710 - accuracy: 0.6172 - val_loss: 0.1562 - val_accuracy: 0.5972\n","Epoch 10/100\n","32/32 [==============================] - 29s 900ms/step - loss: 0.1564 - accuracy: 0.6367 - val_loss: 0.1677 - val_accuracy: 0.4861\n","Epoch 11/100\n","32/32 [==============================] - 29s 898ms/step - loss: 0.1439 - accuracy: 0.6914 - val_loss: 0.1802 - val_accuracy: 0.4444\n","Epoch 12/100\n","32/32 [==============================] - 29s 893ms/step - loss: 0.1367 - accuracy: 0.7078 - val_loss: 0.1550 - val_accuracy: 0.6250\n","Epoch 13/100\n","32/32 [==============================] - 29s 898ms/step - loss: 0.1199 - accuracy: 0.7783 - val_loss: 0.1362 - val_accuracy: 0.7222\n","Epoch 14/100\n","32/32 [==============================] - 29s 893ms/step - loss: 0.1108 - accuracy: 0.7822 - val_loss: 0.1677 - val_accuracy: 0.6250\n","Epoch 15/100\n","32/32 [==============================] - 29s 892ms/step - loss: 0.1031 - accuracy: 0.7998 - val_loss: 0.1387 - val_accuracy: 0.6250\n","Epoch 16/100\n","32/32 [==============================] - 30s 930ms/step - loss: 0.0979 - accuracy: 0.8135 - val_loss: 0.1576 - val_accuracy: 0.6250\n","Epoch 17/100\n","32/32 [==============================] - 29s 895ms/step - loss: 0.0900 - accuracy: 0.8216 - val_loss: 0.1405 - val_accuracy: 0.6528\n","Epoch 18/100\n","32/32 [==============================] - 29s 887ms/step - loss: 0.0831 - accuracy: 0.8392 - val_loss: 0.1391 - val_accuracy: 0.6528\n","Epoch 19/100\n","32/32 [==============================] - 29s 894ms/step - loss: 0.0985 - accuracy: 0.8010 - val_loss: 0.2106 - val_accuracy: 0.5139\n","Epoch 20/100\n","32/32 [==============================] - 29s 893ms/step - loss: 0.0828 - accuracy: 0.8457 - val_loss: 0.1219 - val_accuracy: 0.7778\n","Epoch 21/100\n","32/32 [==============================] - 29s 894ms/step - loss: 0.0761 - accuracy: 0.8594 - val_loss: 0.1341 - val_accuracy: 0.7361\n","Epoch 22/100\n","32/32 [==============================] - 29s 891ms/step - loss: 0.0778 - accuracy: 0.8412 - val_loss: 0.1591 - val_accuracy: 0.5972\n","Epoch 23/100\n","32/32 [==============================] - 29s 895ms/step - loss: 0.0824 - accuracy: 0.8203 - val_loss: 0.1545 - val_accuracy: 0.6111\n","Epoch 24/100\n","32/32 [==============================] - 29s 897ms/step - loss: 0.0737 - accuracy: 0.8574 - val_loss: 0.1339 - val_accuracy: 0.6944\n","Epoch 25/100\n","32/32 [==============================] - 30s 931ms/step - loss: 0.0744 - accuracy: 0.8520 - val_loss: 0.1721 - val_accuracy: 0.6667\n","Epoch 26/100\n","32/32 [==============================] - 29s 895ms/step - loss: 0.0682 - accuracy: 0.8608 - val_loss: 0.1695 - val_accuracy: 0.6528\n","Epoch 27/100\n","32/32 [==============================] - 29s 898ms/step - loss: 0.0602 - accuracy: 0.8848 - val_loss: 0.1476 - val_accuracy: 0.6806\n","Epoch 28/100\n","32/32 [==============================] - 29s 892ms/step - loss: 0.0639 - accuracy: 0.8765 - val_loss: 0.1531 - val_accuracy: 0.6528\n","Epoch 29/100\n","32/32 [==============================] - 29s 889ms/step - loss: 0.0630 - accuracy: 0.8701 - val_loss: 0.1305 - val_accuracy: 0.8056\n","Epoch 30/100\n","32/32 [==============================] - 29s 888ms/step - loss: 0.0587 - accuracy: 0.8877 - val_loss: 0.1038 - val_accuracy: 0.8056\n","Epoch 31/100\n","32/32 [==============================] - 29s 886ms/step - loss: 0.0540 - accuracy: 0.8922 - val_loss: 0.1186 - val_accuracy: 0.7778\n","Epoch 32/100\n","32/32 [==============================] - 29s 891ms/step - loss: 0.0514 - accuracy: 0.9014 - val_loss: 0.1759 - val_accuracy: 0.5556\n","Epoch 33/100\n","32/32 [==============================] - 30s 925ms/step - loss: 0.0575 - accuracy: 0.8922 - val_loss: 0.1493 - val_accuracy: 0.6944\n","Epoch 34/100\n","32/32 [==============================] - 29s 890ms/step - loss: 0.0526 - accuracy: 0.8941 - val_loss: 0.1146 - val_accuracy: 0.8194\n","Epoch 35/100\n","32/32 [==============================] - 29s 892ms/step - loss: 0.0540 - accuracy: 0.9014 - val_loss: 0.0926 - val_accuracy: 0.8333\n","Epoch 36/100\n","32/32 [==============================] - 29s 889ms/step - loss: 0.0522 - accuracy: 0.8892 - val_loss: 0.1341 - val_accuracy: 0.7500\n","Epoch 37/100\n","32/32 [==============================] - 29s 891ms/step - loss: 0.0536 - accuracy: 0.9010 - val_loss: 0.1192 - val_accuracy: 0.8056\n","Epoch 38/100\n","32/32 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9141\n","Reached 90% accuracy so cancelling training!\n","32/32 [==============================] - 29s 896ms/step - loss: 0.0467 - accuracy: 0.9141 - val_loss: 0.1253 - val_accuracy: 0.8056\n"]}],"source":["# ========================================================================================\n","# PROBLEM B3\n","#\n","# Build a CNN based classifier for Rock-Paper-Scissors dataset.\n","# Your input layer should accept 150x150 with 3 bytes color as the input shape.\n","# This is unlabeled data, use ImageDataGenerator to automatically label it.\n","# Don't use lambda layers in your model.\n","#\n","# The dataset used in this problem is created by Laurence Moroney (laurencemoroney.com).\n","#\n","# Desired accuracy AND validation_accuracy > 83%\n","# ========================================================================================\n","\n","import urllib.request\n","import zipfile\n","import tensorflow as tf\n","import os\n","from keras_preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adam\n","\n","\n","def solution_B3():\n","    data_url = 'https://github.com/dicodingacademy/assets/releases/download/release-rps/rps.zip'\n","    urllib.request.urlretrieve(data_url, 'rps.zip')\n","    local_file = 'rps.zip'\n","    zip_ref = zipfile.ZipFile(local_file, 'r')\n","    zip_ref.extractall('data/')\n","    zip_ref.close()\n","\n","    TRAINING_DIR = \"data/rps/\"\n","\n","    training_datagen = ImageDataGenerator (rescale = 1./255,\n","                                       horizontal_flip = True,\n","                                       vertical_flip = True,\n","                                       rotation_range = 20,\n","                                       width_shift_range = 0.2,\n","                                       height_shift_range = 0.2,\n","                                       shear_range = 0.2,\n","                                       fill_mode=\"nearest\",\n","                                       validation_split = 0.1,\n","                                       )\n","    train_generator = training_datagen.flow_from_directory(TRAINING_DIR,\n","                                                      batch_size = 32,\n","                                                      target_size = (150,150),\n","                                                      class_mode = \"categorical\",\n","                                                      subset = \"training\"\n","                                                      )\n","\n","    validation_generator = training_datagen.flow_from_directory(TRAINING_DIR,\n","                                                         batch_size = 9,\n","                                                         target_size = (150,150),\n","                                                         class_mode = \"categorical\",\n","                                                         subset = \"validation\")\n","\n","    # YOUR IMAGE SIZE SHOULD BE 150x150\n","    # Make sure you used \"categorical\"\n","    class CustomCallback(tf.keras.callbacks.Callback):\n","        def on_epoch_end(self, epoch, logs=None):\n","          if (logs.get(\"accuracy\")>0.91):\n","            print(\"\\nReached 90% accuracy so cancelling training!\")\n","            self.model.stop_training = True\n","    callbacks = CustomCallback()\n","    model = tf.keras.models.Sequential([\n","                                    tf.keras.layers.Conv2D(16,(3,3),input_shape =(150,150,3),activation = \"relu\"),\n","                                    tf.keras.layers.MaxPooling2D(2,2),\n","                                    tf.keras.layers.Conv2D(32,(3,3),activation = \"relu\"),\n","                                    tf.keras.layers.MaxPooling2D(2,2),\n","                                    tf.keras.layers.Conv2D(64,(3,3),activation = \"relu\"),\n","                                    tf.keras.layers.MaxPooling2D(2,2),\n","                                    tf.keras.layers.Conv2D(64,(3,3),activation = \"relu\"),\n","                                    tf.keras.layers.MaxPooling2D(2,2),\n","                                    tf.keras.layers.Flatten(),\n","                                    tf.keras.layers.Dropout(0.5),\n","                                    tf.keras.layers.Dense(512,activation=\"relu\"),                                    \n","                                    tf.keras.layers.Dense(3,activation = \"softmax\")\n","                          ])\n","    model.summary()\n","    model.compile(loss  = \"mean_squared_error\",\n","              optimizer= Adam(learning_rate = 0.0001),\n","              metrics = [\"accuracy\"])\n","\n","    model.fit(train_generator,\n","          validation_data  = validation_generator,\n","          steps_per_epoch = 32,\n","          validation_steps = 8, \n","          epochs = 100,\n","          callbacks = callbacks)\n","    return model\n","\n","\n","# The code below is to save your model as a .h5 file.\n","# It will be saved automatically in your Submission folder.\n","if __name__ == '__main__':\n","    # DO NOT CHANGE THIS CODE\n","    model=solution_B3()\n","    model.save(\"model_B3.h5\")\n"]},{"cell_type":"code","source":[""],"metadata":{"id":"lm_IAn32obKH"},"execution_count":null,"outputs":[]}]}